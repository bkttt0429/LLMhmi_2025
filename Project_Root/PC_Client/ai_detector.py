import cv2
import time
import numpy as np
import torch

# === CUDA æ•ˆèƒ½å„ªåŒ– ===
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True  # è‡ªå‹•å°‹æ‰¾æœ€ä½³å·ç©æ¼”ç®—æ³•
    torch.backends.cudnn.deterministic = False  # å…è¨±éç¢ºå®šæ€§æ¼”ç®—æ³•ä»¥æå‡é€Ÿåº¦
    print("âœ… CUDA cuDNN åŠ é€Ÿå·²å•Ÿç”¨")

# å˜—è©¦åŒ¯å…¥ YOLO
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£ ultralyticsã€‚è«‹åŸ·è¡Œ 'pip install ultralytics'")
    YOLO_AVAILABLE = False

class ObjectDetector:
    def __init__(self, model_path='./yolov8n.pt'):
        self.model = None
        self.enabled = False
        self.frame_count = 0
        self.total_inference_time = 0
        
        # === è£ç½®é¸æ“‡èˆ‡è©³ç´°è³‡è¨Š ===
        self.device = self._select_device()
        
        # === æ§åˆ¶åƒæ•¸ ===
        self.base_v = 0.6          # åŸºç¤é€Ÿåº¦
        self.max_w = 2.0           # æœ€å¤§è§’é€Ÿåº¦
        self.conf_th = 0.4         # ä¿¡å¿ƒåº¦é–¾å€¼
        self.target_class = None   # ç›®æ¨™é¡åˆ¥ (None = æ‰€æœ‰é¡åˆ¥)
        
        # === æ•ˆèƒ½å„ªåŒ–åƒæ•¸ ===
        self.skip_frames = 0       # è·³å¹€è¨ˆæ•¸å™¨ (0 = æ¯å¹€éƒ½è™•ç†)
        self.process_every_n = 1   # æ¯ N å¹€è™•ç†ä¸€æ¬¡ (1 = ä¸è·³å¹€)
        self.input_size = 640      # YOLO è¼¸å…¥å°ºå¯¸ (320/640/1280)
        
        # === æ¨¡å‹è¼‰å…¥ ===
        if YOLO_AVAILABLE:
            self._load_model(model_path)

    def _select_device(self):
        """æ™ºèƒ½é¸æ“‡é‹ç®—è£ç½®"""
        if torch.cuda.is_available():
            device = 'cuda'
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            compute_capability = torch.cuda.get_device_capability(0)
            
            print(f"ğŸš€ AI Device: NVIDIA CUDA")
            print(f"   â””â”€ GPU: {gpu_name}")
            print(f"   â””â”€ VRAM: {gpu_memory:.1f} GB")
            print(f"   â””â”€ CUDA Version: {torch.version.cuda}")
            print(f"   â””â”€ Compute Capability: {compute_capability[0]}.{compute_capability[1]}")
            print(f"   â””â”€ cuDNN Benchmark: {'Enabled' if torch.backends.cudnn.benchmark else 'Disabled'}")
            
            # æ ¹æ“š VRAM èª¿æ•´æ‰¹æ¬¡å¤§å°å»ºè­°
            if gpu_memory < 4:
                print("   â””â”€ âš ï¸ ä½ VRAM æª¢æ¸¬ï¼Œå»ºè­°ä½¿ç”¨ yolov8n.pt")
                self.input_size = 320
            elif gpu_memory >= 8:
                print("   â””â”€ âœ… å……è¶³ VRAMï¼Œå¯ä½¿ç”¨æ›´å¤§æ¨¡å‹")
                self.input_size = 640
            
            # æ¸…ç©º GPU å¿«å–
            torch.cuda.empty_cache()
            print("   â””â”€ GPU å¿«å–å·²æ¸…ç©º")
            
        elif torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸš€ AI Device: Apple Metal (MPS)")
            print("   â””â”€ Optimized for Apple Silicon")
            
        else:
            device = 'cpu'
            print("âš ï¸ AI Device: CPU")
            print("   â””â”€ å»ºè­°: ä½¿ç”¨ GPU ä»¥ç²å¾—æ›´å¥½æ•ˆèƒ½")
            self.process_every_n = 2  # CPU æ¨¡å¼è‡ªå‹•è·³å¹€
            
        return device

    def _load_model(self, model_path):
        """è¼‰å…¥ YOLO æ¨¡å‹"""
        print(f"\n[AI] å˜—è©¦è¼‰å…¥æ¨¡å‹: {model_path}...")
        
        try:
            self.model = YOLO(model_path)
            self.model.to(self.device)  # æ˜ç¢ºç§»å‹•åˆ°ç›®æ¨™è£ç½®
            print(f"[AI] âœ… {model_path} è¼‰å…¥æˆåŠŸ")
            self.enabled = True
            
            # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
            if hasattr(self.model, 'names'):
                print(f"[AI] å¯åµæ¸¬é¡åˆ¥æ•¸: {len(self.model.names)}")
                print(f"[AI] ç¯„ä¾‹é¡åˆ¥: {list(self.model.names.values())[:5]}...")
                
        except FileNotFoundError:
            print(f"[AI] âš ï¸ {model_path} ä¸å­˜åœ¨")
            print("[AI] å˜—è©¦ä½¿ç”¨ yolov8n.pt (è‡ªå‹•ä¸‹è¼‰)...")
            try:
                self.model = YOLO('yolov8n.pt')
                self.model.to(self.device)
                self.enabled = True
                print("[AI] âœ… yolov8n.pt è¼‰å…¥æˆåŠŸ")
            except Exception as e2:
                print(f"[AI] âŒ ç„¡æ³•è¼‰å…¥å‚™ç”¨æ¨¡å‹: {e2}")
                
        except Exception as e:
            print(f"[AI] âŒ è¼‰å…¥å¤±æ•—: {e}")
            print("[AI] å˜—è©¦é™ç´šåˆ° CPU...")
            try:
                self.device = 'cpu'
                self.model = YOLO(model_path)
                self.enabled = True
                print("[AI] âœ… CPU æ¨¡å¼è¼‰å…¥æˆåŠŸ")
            except:
                print("[AI] âŒ åš´é‡éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥ä»»ä½•æ¨¡å‹")

    def decide_control(self, result, img_w, img_h):
        """
        è¨ˆç®—è‡ªå‹•é§•é§›æ§åˆ¶é‡
        
        Returns:
            (v, w): ç·šé€Ÿåº¦å’Œè§’é€Ÿåº¦
        """
        boxes = result.boxes
        if boxes is None or len(boxes) == 0:
            return 0.0, 0.0

        # GPU -> CPU è½‰æ› (å¿…è¦æ™‚)
        xyxy = boxes.xyxy.cpu().numpy() if boxes.xyxy.is_cuda else boxes.xyxy.numpy()
        cls = boxes.cls.cpu().numpy() if boxes.cls.is_cuda else boxes.cls.numpy()
        conf = boxes.conf.cpu().numpy() if boxes.conf.is_cuda else boxes.conf.numpy()

        # å°‹æ‰¾æœ€ä½³ç›®æ¨™ (é¢ç©æœ€å¤§ + ç¬¦åˆæ¢ä»¶)
        best_box = None
        best_score = -1

        for (x1, y1, x2, y2), c, s in zip(xyxy, cls, conf):
            # éæ¿¾æ¢ä»¶
            if s < self.conf_th: 
                continue
            if self.target_class is not None and int(c) != self.target_class: 
                continue

            # è¨ˆç®—é¢ç©
            area = (x2 - x1) * (y2 - y1)
            if area > best_score:
                best_score = area
                best_box = (x1, y1, x2, y2)

        if best_box is None:
            return 0.0, 0.0

        # è¨ˆç®—æ§åˆ¶é‡
        x1, y1, x2, y2 = best_box
        cx = 0.5 * (x1 + x2)  # ä¸­å¿ƒ X
        h = y2 - y1           # é«˜åº¦

        # æ­£è¦åŒ–åç§»é‡ [-1, 1]
        x_offset = (cx - img_w / 2) / (img_w / 2)
        
        # ç‰©é«”å¤§å°æ¯”ä¾‹
        size_ratio = h / img_h

        # è§’é€Ÿåº¦æ§åˆ¶ (æ ¹æ“š X åç§»)
        w = x_offset * self.max_w
        
        # ç·šé€Ÿåº¦æ§åˆ¶ (ç‰©é«”è¶Šè¿‘è¶Šæ…¢)
        v = self.base_v * max(0.0, 1.0 - size_ratio)
        
        # ç·Šæ€¥åœæ­¢ (ç‰©é«”å¤ªè¿‘)
        if size_ratio > 0.6:
            v = 0.0

        return v, w

    def detect(self, frame):
        """
        æ ¸å¿ƒåµæ¸¬æ–¹æ³•
        
        Args:
            frame: BGR åœ–åƒ (numpy array)
            
        Returns:
            (annotated_frame, detections_list, control_cmd)
        """
        # é˜²å‘†æª¢æŸ¥
        if not self.enabled or self.model is None:
            return frame, [], (0.0, 0.0)

        # è·³å¹€å„ªåŒ– (æå‡ FPS)
        self.skip_frames += 1
        if self.skip_frames < self.process_every_n:
            return frame, [], (0.0, 0.0)
        self.skip_frames = 0

        # === é–‹å§‹æ¨è«– ===
        start_time = time.time()
        h, w_img = frame.shape[:2]
        
        try:
            # 1. YOLO æ¨è«– (ä½¿ç”¨ track æ¨¡å¼ä¿æŒ ID)
            results = self.model.track(
                frame, 
                device=self.device,
                persist=True,           # ä¿æŒè¿½è¹¤ ID
                conf=self.conf_th,      # ä¿¡å¿ƒåº¦é–¾å€¼
                verbose=False,          # ä¸é¡¯ç¤ºè©³ç´° log
                imgsz=self.input_size,  # è¼¸å…¥å°ºå¯¸
                half=self.device=='cuda' # GPU ä½¿ç”¨åŠç²¾åº¦åŠ é€Ÿ
            )
            result = results[0]
            
            # 2. ç¹ªè£½çµæœ (YOLO å…§å»ºæœ€å¿«)
            annotated_frame = result.plot()
            
            # 3. è¨ˆç®—æ§åˆ¶æŒ‡ä»¤
            v, ang_w = self.decide_control(result, w_img, h)
            
            # 4. æ•´ç†åµæ¸¬è³‡è¨Š
            detections = []
            if result.boxes:
                for box in result.boxes:
                    try:
                        cls_id = int(box.cls[0].item())
                        conf = float(box.conf[0].item())
                        
                        # å®‰å…¨ç²å–é¡åˆ¥åç¨±
                        if hasattr(self.model, 'names') and cls_id in self.model.names:
                            cls_name = self.model.names[cls_id]
                        else:
                            cls_name = f"class_{cls_id}"
                        
                        detections.append({
                            "class": cls_name,
                            "confidence": conf,
                            "id": int(box.id[0].item()) if box.id is not None else -1
                        })
                    except Exception as e:
                        print(f"[AI] è§£æåµæ¸¬çµæœéŒ¯èª¤: {e}")
            
            # 5. æ•ˆèƒ½çµ±è¨ˆ
            inference_time = time.time() - start_time
            self.frame_count += 1
            self.total_inference_time += inference_time
            
            fps = 1.0 / inference_time if inference_time > 0 else 0
            avg_fps = self.frame_count / self.total_inference_time if self.total_inference_time > 0 else 0
            
            # 6. é¡¯ç¤º HUD
            info_lines = [
                f"FPS: {fps:.1f} (Avg: {avg_fps:.1f})",
                f"Device: {self.device.upper()}",
                f"Objects: {len(detections)}",
                f"Control: v={v:.2f} w={ang_w:.2f}"
            ]
            
            y_offset = 30
            for line in info_lines:
                cv2.putText(
                    annotated_frame, line, 
                    (10, y_offset), 
                    cv2.FONT_HERSHEY_SIMPLEX, 
                    0.6, (0, 255, 0), 2
                )
                y_offset += 25
            
            return annotated_frame, detections, (v, ang_w)
            
        except Exception as e:
            print(f"[AI] æ¨è«–éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return frame, [], (0.0, 0.0)

    def set_target_class(self, class_name):
        """è¨­å®šè¿½è¹¤ç›®æ¨™é¡åˆ¥"""
        if self.model and hasattr(self.model, 'names'):
            for idx, name in self.model.names.items():
                if name.lower() == class_name.lower():
                    self.target_class = idx
                    print(f"[AI] ç›®æ¨™è¨­å®šç‚º: {name} (ID: {idx})")
                    return True
        print(f"[AI] æ‰¾ä¸åˆ°é¡åˆ¥: {class_name}")
        return False
    
    def reset_target(self):
        """æ¸…é™¤ç›®æ¨™é™åˆ¶"""
        self.target_class = None
        print("[AI] åµæ¸¬æ‰€æœ‰é¡åˆ¥")

    def get_stats(self):
        """ç²å–æ•ˆèƒ½çµ±è¨ˆ"""
        if self.frame_count == 0:
            return {"fps": 0, "frames": 0}
        
        avg_fps = self.frame_count / self.total_inference_time if self.total_inference_time > 0 else 0
        
        stats = {
            "fps": avg_fps,
            "frames": self.frame_count,
            "device": self.device,
            "model": str(self.model.model_name) if hasattr(self.model, 'model_name') else "unknown"
        }
        
        # GPU å°ˆå±¬è³‡è¨Š
        if self.device == 'cuda' and torch.cuda.is_available():
            stats.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**2:.1f} MB",
                "gpu_memory_reserved": f"{torch.cuda.memory_reserved(0) / 1024**2:.1f} MB",
                "cudnn_benchmark": torch.backends.cudnn.benchmark
            })
        
        return stats
    
    def optimize_for_speed(self):
        """æ¥µé€Ÿæ¨¡å¼å„ªåŒ–"""
        print("[AI] ğŸš€ å•Ÿç”¨æ¥µé€Ÿæ¨¡å¼...")
        self.conf_th = 0.5  # æé«˜é–¾å€¼æ¸›å°‘èª¤æª¢
        self.input_size = 320  # é™ä½è¼¸å…¥è§£æåº¦
        self.process_every_n = 1  # ä¸è·³å¹€
        
        if self.device == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            print("[AI] âœ… CUDA åŠ é€Ÿå„ªåŒ–å®Œæˆ")
    
    def optimize_for_accuracy(self):
        """ç²¾ç¢ºæ¨¡å¼å„ªåŒ–"""
        print("[AI] ğŸ¯ å•Ÿç”¨ç²¾ç¢ºæ¨¡å¼...")
        self.conf_th = 0.3  # é™ä½é–¾å€¼æé«˜å¬å›ç‡
        self.input_size = 640  # æé«˜è¼¸å…¥è§£æåº¦
        self.process_every_n = 1  # ä¸è·³å¹€
        print("[AI] âœ… ç²¾ç¢ºæ¨¡å¼è¨­å®šå®Œæˆ")