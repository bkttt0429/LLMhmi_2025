"""
CUDA è¨ºæ–·è…³æœ¬ - æª¢æŸ¥ PyTorch CUDA é…ç½®
"""
import torch
import sys

print("=" * 60)
print("ğŸ” PyTorch CUDA è¨ºæ–·")
print("=" * 60)

print(f"\nğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"ğŸ Python ç‰ˆæœ¬: {sys.version}")

print(f"\nğŸ® CUDA å¯ç”¨æ€§:")
print(f"   torch.cuda.is_available(): {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"\nâœ… CUDA å·²å•Ÿç”¨")
    print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        print(f"\n   GPU {i}:")
        print(f"      åç¨±: {torch.cuda.get_device_name(i)}")
        props = torch.cuda.get_device_properties(i)
        print(f"      VRAM: {props.total_memory / 1024**3:.1f} GB")
        print(f"      Compute Capability: {props.major}.{props.minor}")
    
    # æ¸¬è©¦ CUDA æ“ä½œ
    try:
        x = torch.zeros(1).cuda()
        print(f"\nâœ… CUDA å¼µé‡å‰µå»ºæˆåŠŸ")
        del x
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"\nâŒ CUDA å¼µé‡å‰µå»ºå¤±æ•—: {e}")
        
else:
    print(f"\nâŒ CUDA ä¸å¯ç”¨")
    print(f"\nå¯èƒ½çš„åŸå› :")
    print(f"   1. PyTorch æœªç·¨è­¯ CUDA æ”¯æ´ (CPU-only ç‰ˆæœ¬)")
    print(f"   2. CUDA é©…å‹•æœªå®‰è£æˆ–ç‰ˆæœ¬ä¸åŒ¹é…")
    print(f"   3. ç’°å¢ƒè®Šæ•¸è¨­å®šéŒ¯èª¤")
    print(f"\næª¢æŸ¥æ­¥é©Ÿ:")
    print(f"   1. åŸ·è¡Œ: python -c \"import torch; print(torch.version.cuda)\"")
    print(f"   2. åŸ·è¡Œ: nvidia-smi")
    print(f"   3. é‡æ–°å®‰è£ PyTorch with CUDA:")
    print(f"      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")

print("\n" + "=" * 60)
