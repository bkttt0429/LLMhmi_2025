#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""GPU ç‹€æ…‹æª¢æ¸¬å·¥å…·"""

import torch
import sys

print("=" * 60)
print("ğŸ” PyTorch GPU æª¢æ¸¬å ±å‘Š")
print("=" * 60)

# 1. CUDA å¯ç”¨æ€§
cuda_available = torch.cuda.is_available()
print(f"\nâœ… CUDA å¯ç”¨: {cuda_available}")

if not cuda_available:
    print("\nâŒ CUDA ä¸å¯ç”¨ï¼Œå¯èƒ½åŸå› :")
    print("   1. æœªå®‰è£ NVIDIA GPU é©…å‹•ç¨‹å¼")
    print("   2. æœªå®‰è£ CUDA Toolkit")
    print("   3. PyTorch ç‰ˆæœ¬ä¸æ”¯æ´ CUDA (CPU-only)")
    print("\nğŸ’¡ ä¿®å¾©æ–¹æ³•:")
    print("   - æª¢æŸ¥é©…å‹•: nvidia-smi")
    print("   - é‡æ–°å®‰è£ PyTorch: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121")
    sys.exit(1)

# 2. CUDA ç‰ˆæœ¬
print(f"   â””â”€ CUDA ç‰ˆæœ¬: {torch.version.cuda}")

# 3. cuDNN ç‰ˆæœ¬
if torch.backends.cudnn.is_available():
    print(f"   â””â”€ cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"   â””â”€ cuDNN Benchmark: {torch.backends.cudnn.benchmark}")

# 4. GPU æ•¸é‡
gpu_count = torch.cuda.device_count()
print(f"\nğŸ® GPU æ•¸é‡: {gpu_count}")

# 5. GPU è©³ç´°è³‡è¨Š
for i in range(gpu_count):
    print(f"\nğŸ“Š GPU {i} è©³ç´°è³‡è¨Š:")
    print(f"   â”œâ”€ åç¨±: {torch.cuda.get_device_name(i)}")
    
    props = torch.cuda.get_device_properties(i)
    print(f"   â”œâ”€ ç¸½è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
    print(f"   â”œâ”€ å¤šè™•ç†å™¨æ•¸é‡: {props.multi_processor_count}")
    print(f"   â”œâ”€ Compute Capability: {props.major}.{props.minor}")
    
    # å³æ™‚è¨˜æ†¶é«”ä½¿ç”¨
    allocated = torch.cuda.memory_allocated(i) / 1024**2
    reserved = torch.cuda.memory_reserved(i) / 1024**2
    print(f"   â”œâ”€ å·²åˆ†é…è¨˜æ†¶é«”: {allocated:.2f} MB")
    print(f"   â””â”€ å·²ä¿ç•™è¨˜æ†¶é«”: {reserved:.2f} MB")

# 6. æ¸¬è©¦ GPU é‹ç®—
print("\nğŸ§ª æ¸¬è©¦ GPU é‹ç®—...")
try:
    # å‰µå»ºæ¸¬è©¦å¼µé‡
    x = torch.rand(1000, 1000).cuda()
    y = torch.rand(1000, 1000).cuda()
    
    # åŸ·è¡ŒçŸ©é™£ä¹˜æ³•
    import time
    start = time.time()
    z = torch.matmul(x, y)
    torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ
    elapsed = time.time() - start
    
    print(f"   âœ… GPU çŸ©é™£é‹ç®—æˆåŠŸ (1000x1000)")
    print(f"   â””â”€ è€—æ™‚: {elapsed*1000:.2f} ms")
    
    # æ¸…ç†
    del x, y, z
    torch.cuda.empty_cache()
    
except Exception as e:
    print(f"   âŒ GPU é‹ç®—å¤±æ•—: {e}")

# 7. PyTorch ç‰ˆæœ¬è³‡è¨Š
print(f"\nğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"   â””â”€ å»ºç½®ç‰ˆæœ¬: {torch.version.debug if hasattr(torch.version, 'debug') else 'N/A'}")

# 8. ç’°å¢ƒå»ºè­°
print("\nğŸ’¡ YOLOv13 æœ€ä½³é…ç½®:")
if gpu_count > 0:
    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if vram < 4:
        print("   âš ï¸ VRAM < 4GBï¼Œå»ºè­°:")
        print("      - ä½¿ç”¨ yolov8n.pt æˆ– yolov8s.pt")
        print("      - é™ä½è¼¸å…¥å°ºå¯¸ (320)")
    elif vram >= 8:
        print("   âœ… VRAM >= 8GBï¼Œå¯ä½¿ç”¨:")
        print("      - yolov13l.pt æˆ– yolov13x.pt")
        print("      - è¼¸å…¥å°ºå¯¸ 640 æˆ–æ›´é«˜")
    else:
        print("   âœ… VRAM 4-8GBï¼Œå»ºè­°:")
        print("      - yolov8m.pt æˆ– yolov13s.pt")
        print("      - è¼¸å…¥å°ºå¯¸ 640")

print("\n" + "=" * 60)
print("âœ… æª¢æ¸¬å®Œæˆ")
print("=" * 60)
