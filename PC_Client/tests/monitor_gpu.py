#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å³æ™‚ GPU ä½¿ç”¨ç‡ç›£æ§å·¥å…·"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import time
import sys
from datetime import datetime

def clear_screen():
    """æ¸…é™¤çµ‚ç«¯ç•«é¢"""
    os.system('cls' if os.name == 'nt' else 'clear')

def format_bytes(bytes_val):
    """æ ¼å¼åŒ–ä½å…ƒçµ„å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_val < 1024.0:
            return f"{bytes_val:.2f} {unit}"
        bytes_val /= 1024.0
    return f"{bytes_val:.2f} TB"

def get_gpu_usage():
    """ç²å– GPU ä½¿ç”¨è³‡è¨Š"""
    if not torch.cuda.is_available():
        return None
    
    device = 0
    props = torch.cuda.get_device_properties(device)
    
    # è¨˜æ†¶é«”è³‡è¨Š
    total_memory = props.total_memory
    allocated = torch.cuda.memory_allocated(device)
    reserved = torch.cuda.memory_reserved(device)
    free = total_memory - allocated
    
    # è¨ˆç®—ä½¿ç”¨ç‡
    usage_percent = (allocated / total_memory) * 100
    
    return {
        'name': torch.cuda.get_device_name(device),
        'total': total_memory,
        'allocated': allocated,
        'reserved': reserved,
        'free': free,
        'usage_percent': usage_percent,
        'compute_capability': f"{props.major}.{props.minor}",
        'multi_processors': props.multi_processor_count
    }

def draw_progress_bar(percent, width=50):
    """ç¹ªè£½é€²åº¦æ¢"""
    filled = int(width * percent / 100)
    bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
    
    # æ ¹æ“šä½¿ç”¨ç‡é¸æ“‡é¡è‰²ï¼ˆANSI é¡è‰²ç¢¼ï¼‰
    if percent < 50:
        color = '\033[92m'  # ç¶ è‰²
    elif percent < 80:
        color = '\033[93m'  # é»ƒè‰²
    else:
        color = '\033[91m'  # ç´…è‰²
    
    reset = '\033[0m'
    return f"{color}{bar}{reset} {percent:.1f}%"

def monitor_gpu(interval=1.0):
    """ä¸»ç›£æ§å¾ªç’°"""
    print("ğŸš€ å•Ÿå‹• GPU ç›£æ§...")
    print("æŒ‰ Ctrl+C åœæ­¢\n")
    time.sleep(1)
    
    try:
        while True:
            clear_screen()
            
            # æ¨™é¡Œ
            print("=" * 80)
            print(f"ğŸ® GPU å³æ™‚ç›£æ§ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 80)
            
            # ç²å– GPU è³‡è¨Š
            gpu_info = get_gpu_usage()
            
            if gpu_info is None:
                print("\nâŒ CUDA ä¸å¯ç”¨")
                break
            
            # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
            print(f"\nğŸ“Š GPU è³‡è¨Š")
            print(f"   åç¨±: {gpu_info['name']}")
            print(f"   Compute Capability: {gpu_info['compute_capability']}")
            print(f"   å¤šè™•ç†å™¨æ•¸é‡: {gpu_info['multi_processors']}")
            
            # è¨˜æ†¶é«”ä½¿ç”¨
            print(f"\nğŸ’¾ VRAM ä½¿ç”¨ç‹€æ³")
            print(f"   ç¸½å®¹é‡: {format_bytes(gpu_info['total'])}")
            print(f"   å·²ä½¿ç”¨: {format_bytes(gpu_info['allocated'])}")
            print(f"   å·²ä¿ç•™: {format_bytes(gpu_info['reserved'])}")
            print(f"   å¯ç”¨:   {format_bytes(gpu_info['free'])}")
            
            # ä½¿ç”¨ç‡é€²åº¦æ¢
            print(f"\nğŸ“ˆ ä½¿ç”¨ç‡")
            print(f"   {draw_progress_bar(gpu_info['usage_percent'])}")
            
            # PyTorch è³‡è¨Š
            print(f"\nğŸ”§ PyTorch ç’°å¢ƒ")
            print(f"   ç‰ˆæœ¬: {torch.__version__}")
            print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   cuDNN Benchmark: {torch.backends.cudnn.benchmark}")
            
            # æç¤º
            print("\n" + "=" * 80)
            print(f"æ›´æ–°é–“éš”: {interval}s | æŒ‰ Ctrl+C åœæ­¢")
            
            time.sleep(interval)
            
    except KeyboardInterrupt:
        print("\n\nâœ… ç›£æ§å·²åœæ­¢")
        sys.exit(0)

if __name__ == "__main__":
    # æª¢æŸ¥åƒæ•¸
    interval = 1.0
    if len(sys.argv) > 1:
        try:
            interval = float(sys.argv[1])
        except ValueError:
            print("âš ï¸ ç„¡æ•ˆçš„é–“éš”æ™‚é–“ï¼Œä½¿ç”¨é è¨­å€¼ 1.0 ç§’")
    
    monitor_gpu(interval)
